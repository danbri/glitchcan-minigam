name: LLM checks & auto-exec

on:
  push:
    paths:
      - '**/*.fink.js'
      - 'reports/autoexec.bot/*-input.txt'
  pull_request:
    paths:
      - '**/*.fink.js'
      - 'reports/autoexec.bot/*-input.txt'

jobs:
  llm:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      # 1 ─ Checkout repo
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2 ─ Cache llama.cpp build + model
      - name: Cache llama.cpp & model
        uses: actions/cache@v4
        with:
          key: qwen15-code-7b-v6-${{ runner.os }}
          path: |
            ~/.cache/llama
            ~/llama.cpp

      # 3 ─ Build llama.cpp (only on cache miss), including shared library
      - name: Build llama.cpp
        if: steps.cache.outputs.cache-hit != 'true'
        run: |
          sudo apt-get update -y
          sudo apt-get install -y build-essential cmake libcurl4-openssl-dev
          git clone --depth 1 https://github.com/ggerganov/llama.cpp ~/llama.cpp
          cmake -S ~/llama.cpp -B ~/llama.cpp/build \
                -DLLAMA_BUILD_EXAMPLES=OFF \
                -DLLAMA_SHARED=ON
          cmake --build ~/llama.cpp/build --config Release -j$(nproc)

      # 3a ─ Debug: list everything under ~/llama.cpp/build so we see where libllama.so is
      - name: Debug, show llama.cpp/build contents
        if: steps.cache.outputs.cache-hit != 'true'
        run: |
          echo "==== CONTENTS OF ~/llama.cpp/build ===="
          ls -R ~/llama.cpp/build || true
          echo "======================================="

      # 4 ─ Fetch CodeQwen-1.5-7B-Chat-Q4_0 (once, then cached)
      - name: Fetch model
        run: |
          set -euo pipefail
          MODEL_DIR="$HOME/.cache/llama"
          FILE="$MODEL_DIR/codeqwen-1_5-7b-chat-q4_0.gguf"
          URL="https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat-GGUF/resolve/main/codeqwen-1_5-7b-chat-q4_0.gguf"
          mkdir -p "$MODEL_DIR"
          [ -s "$FILE" ] || rm -f "$FILE"
          if [ ! -f "$FILE" ]; then
            echo "Downloading CodeQwen-1.5-7B-Chat Q4_0 …"
            curl -L --fail --retry 3 -o "$FILE" "$URL"
          fi

      # 5 ─ Install latest ctransformers from GitHub
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -q cmake
          pip install -q "git+https://github.com/marella/ctransformers.git"

      # 6 ─ Lint / Test with LLM (discover libllama.so recursively + print candidates)
      - name: Lint / Test with LLM
        env:
          MODEL_PATH: /home/runner/.cache/llama/codeqwen-1_5-7b-chat-q4_0.gguf
        run: |
          python - <<'PY'
          import os, glob
          from pathlib import Path
          from ctransformers import AutoModelForCausalLM

          model_path = os.environ["MODEL_PATH"]
          build_dir = os.path.expanduser("~/llama.cpp/build")

          # Recursively find any libllama*.so under build_dir
          candidates = glob.glob(f"{build_dir}/**/libllama*.so", recursive=True)

          print(">>> libllama.so candidates found:")
          for c in candidates:
              print("   -", c)
          if not candidates:
              raise RuntimeError(f"Could not locate any libllama.so under {build_dir!r}")

          llama_lib = candidates[0]
          print(f">>> Using shared library: {llama_lib}")

          llm = AutoModelForCausalLM.from_pretrained(
              model_path,
              model_type="qwen",
              lib=llama_lib,
              gpu_layers=0,
          )

          for js_file in Path(".").rglob("*.fink.js"):
              code = js_file.read_text()[:4000]
              resp = llm(
                  "<|im_start|>system\nReview JS code.<|im_end|>\n"
                  f"<|im_start|>user\n{code}<|im_end|>\n"
                  "<|im_start|>assistant\n",
                  max_new_tokens=128,
              )
              print(f"\n--- {js_file} ---\n{resp}\n")
          PY

      # 7 ─ Batch-process *-input.txt prompts
      - name: Auto-exec bot
        env:
          MODEL: /home/runner/.cache/llama/codeqwen-1_5-7b-chat-q4_0.gguf
        run: |
          set -euo pipefail
          BIN="$HOME/llama.cpp/build/bin/llama"
          SYS="<|im_start|>system
          You are a concise assistant.<|im_end|>"
          shopt -s nullglob
          for f in reports/autoexec.bot/*-input.txt; do
            [ -f "$f" ] || continue
            USER="<|im_start|>user
            $(cat "$f")<|im_end|>"
            PROMPT="$SYS
            $USER
            <|im_start|>assistant"
            OUT="${f/-input.txt/-output.txt}"
            "$BIN" -m "$MODEL" -p "$PROMPT" -n 256 --temp 0.7 > "$OUT"
            echo "Generated $OUT"
          done

      # 8 ─ Upload generated outputs
      - name: Upload auto-exec outputs
        uses: actions/upload-artifact@v4
        with:
          name: autoexec-reports
          path: reports/autoexec.bot/*-output.txt
