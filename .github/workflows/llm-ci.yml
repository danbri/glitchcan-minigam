name: LLM checks & auto-exec

on:
  push:
    paths:
      - '**/*.fink.js'
      - 'reports/autoexec.bot/*-input.txt'

jobs:
  llm:
    runs-on: ubuntu-latest          # 2 vCPU · 7 GB RAM
    timeout-minutes: 30

    steps:
    # 1 ─ checkout repo --------------------------------------------------------
    - uses: actions/checkout@v4

    # 2 ─ reuse build + model if cached ----------------------------------------
    - name: Cache llama.cpp + model
      uses: actions/cache@v4
      with:
        # bumped “-v2” so we don’t restore the 0-byte file from the bad run
        key: qwen15-code-7b-v2-${{ runner.os }}
        path: |
          ~/.cache/llama
          ~/llama.cpp

    # 3 ─ build llama.cpp (only when cache miss) -------------------------------
    - name: Build llama.cpp
      if: steps.cache.outputs.cache-hit != 'true'
      run: |
        sudo apt-get update -y
        sudo apt-get install -y build-essential cmake libcurl4-openssl-dev
        git clone --depth 1 https://github.com/ggerganov/llama.cpp ~/llama.cpp
        cmake -S ~/llama.cpp -B ~/llama.cpp/build -DLLAMA_BUILD_EXAMPLES=OFF
        cmake --build ~/llama.cpp/build --config Release -j$(nproc)

    # 4 ─ download CodeQwen-1.5-7B-Chat Q4_0 (once, then cached) ---------------
    - name: Fetch model
      run: |
        set -euo pipefail
        MODEL_DIR=$HOME/.cache/llama
        FILE=$MODEL_DIR/codeqwen-1_5-7b-chat-q4_0.gguf
        URL=https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat-GGUF/resolve/main/codeqwen-1_5-7b-chat-q4_0.gguf

        mkdir -p "$MODEL_DIR"
        # if a zero-byte stub snuck in, wipe it
        [ -s "$FILE" ] || rm -f "$FILE"

        if [ ! -f "$FILE" ]; then
          echo "Downloading CodeQwen-1.5-7B-Chat Q4_0 (≈4.5 GB)…"
          curl -L --fail --retry 3 -o "$FILE" "$URL"
        fi

    # 5 — install Python deps for your LLM test
   - name: Install Python deps
     run: |
       pip install --upgrade pip
       pip install -q "ctransformers==0.2.27"

    # 6 ─ run your Python review against changed *.fink.js ----------------------
    - name: Lint/Test with LLM
      env:
        MODEL_PATH: $HOME/.cache/llama/codeqwen-1_5-7b-chat-q4_0.gguf
      run: python tools/llm_check.py

    # 7 ─ batch-process any *-input.txt prompts --------------------------------
    - name: Auto-exec bot
      env:
        MODEL: $HOME/.cache/llama/codeqwen-1_5-7b-chat-q4_0.gguf
      run: |
        set -euo pipefail
        BIN=$HOME/llama.cpp/build/bin/llama
        SYS="<|im_start|>system
        You are a concise assistant that replies in English.<|im_end|>"
        shopt -s nullglob
        for f in reports/autoexec.bot/*-input.txt; do
          [ -f "$f" ] || continue
          USER="<|im_start|>user
          $(cat "$f")<|im_end|>"
          PROMPT="$SYS
          $USER
          <|im_start|>assistant"
          out="${f/-input.txt/-output.txt}"
          "$BIN" -m "$MODEL" -p "$PROMPT" -n 256 --temp 0.7 > "$out"
          echo "Generated $out"
        done

    # 8 ─ upload generated outputs ---------------------------------------------
    - name: Upload auto-exec outputs
      uses: actions/upload-artifact@v4
      with:
        name: autoexec-reports
        path: reports/autoexec.bot/*-output.txt